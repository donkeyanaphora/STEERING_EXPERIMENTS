{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3da3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "\n",
    "# full GPU reset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "cache_dir = (Path.cwd() / \"models\").resolve()\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    # else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "os.environ[\"HF_HOME\"] = str(cache_dir)\n",
    "print(f'Device: {device}')\n",
    "model_card = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_card).to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = model.lm_head.weight.detach()\n",
    "W, d = gamma.shape\n",
    "gamma_bar = torch.mean(gamma, dim=0)\n",
    "centered_gamma = gamma - gamma_bar\n",
    "\n",
    "### compute Cov(gamma) and tranform gamma to g ###\n",
    "cov_gamma = centered_gamma.T @ centered_gamma / W\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(cov_gamma)\n",
    "\n",
    "inv_sqrt_cov_gamma = eigenvectors @ torch.diag(1/torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "sqrt_cov_gamma = eigenvectors @ torch.diag(torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "\n",
    "# gamma is our original head and inv_sqrt_cov_gamma puts us in a causal basis\n",
    "g = gamma @ inv_sqrt_cov_gamma\n",
    "\n",
    "print(model.config.hidden_size)\n",
    "print(g.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenval_min_max = f\"Eigenval min: {eigenvalues.min()}\\nEigenval max: {eigenvalues.max()}\"\n",
    "max_amp = f\"Max amplification (1/sqrt(min)): {1 / torch.sqrt(eigenvalues.min()).item():.1f}\\n\"\n",
    "gamma_min_max = f\"gamma min: {gamma.min()}\\ngamma max: {gamma.max()}\\n\"\n",
    "g_min_max = f\"gamma @ inv_sqrt_cov_gamma min: {g.min()}\\ngamma @ inv_sqrt_cov_gamma max: {g.max()}\\n\"\n",
    "\n",
    "print(eigenval_min_max)\n",
    "print(max_amp)\n",
    "print(gamma_min_max)\n",
    "print(g_min_max)\n",
    "print(f\"gamma dtype: {gamma.dtype}\")\n",
    "print(f\"g dtype: {g.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "concept_df = pd.read_json(\"https://raw.githubusercontent.com/donkeyanaphora/STEERING_EXPERIMENTS/refs/heads/main/data/epistemic_privilege_pairs.json\")\n",
    "\n",
    "a_pairs = []\n",
    "b_pairs = []\n",
    "\n",
    "for idx, row in concept_df.iterrows():\n",
    "    a = [\n",
    "        {\"role\": \"assistant\", \"content\": row.prompt}, \n",
    "        {\"role\": \"user\", \"content\": row.high_sentence}\n",
    "    ]\n",
    "    a_pairs.append(a)\n",
    "\n",
    "    b = [\n",
    "        {\"role\": \"assistant\", \"content\": row.prompt}, \n",
    "        {\"role\": \"user\", \"content\": row.low_sentence}\n",
    "    ]\n",
    "    b_pairs.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_batch = tokenizer.apply_chat_template(\n",
    "    a_pairs,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=False,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    padding=True,\n",
    ").to(device)\n",
    "\n",
    "b_batch = tokenizer.apply_chat_template(\n",
    "    b_pairs,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=False,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    padding=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean_pool(last_hidden, attention_mask):\n",
    "    \"\"\"Pool over all non-padded tokens. Shared content cancels in the diff.\"\"\"\n",
    "    mask = attention_mask.unsqueeze(-1)\n",
    "    return (last_hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "def last_token_pool(last_hidden, attention_mask):\n",
    "    \"\"\"Pool last non-padded token.\"\"\"\n",
    "    idx = attention_mask.sum(dim=1) - 1\n",
    "    return last_hidden[torch.arange(last_hidden.size(0), device=last_hidden.device), idx]\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = model(**a_batch, output_hidden_states=True)\n",
    "    out2 = model(**b_batch, output_hidden_states=True)\n",
    "\n",
    "# simple mean pooling - shared content cancels when we take (a - b)\n",
    "a_emb = masked_mean_pool(out1.hidden_states[-1], a_batch[\"attention_mask\"])\n",
    "b_emb = masked_mean_pool(out2.hidden_states[-1], b_batch[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead264c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify signal quality\n",
    "diffs = a_emb - b_emb\n",
    "concept_dir_raw = diffs.mean(dim=0)\n",
    "\n",
    "# check per-pair alignment with mean direction\n",
    "cos_sims = []\n",
    "for i in range(len(diffs)):\n",
    "    sim = torch.cosine_similarity(diffs[i], concept_dir_raw, dim=0).item()\n",
    "    cos_sims.append(sim)\n",
    "print(f\"Per-pair cosine sims: {[f'{s:.2f}' for s in cos_sims]}\")\n",
    "print(f\"Mean: {sum(cos_sims)/len(cos_sims):.3f}\")\n",
    "print(f\"All positive: {all(s > 0 for s in cos_sims)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concept_dir",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_dir = concept_dir_raw @ sqrt_cov_gamma\n",
    "concept_dir = concept_dir / concept_dir.norm()\n",
    "concept_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteeringHead(torch.nn.Module):\n",
    "    def __init__(self, lm_head_g, sqrt_cov_gamma, concept_dir, alpha=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"lm_head_g\", lm_head_g)\n",
    "        self.register_buffer(\"sqrt_cov_gamma\", sqrt_cov_gamma)\n",
    "        self.register_buffer(\"concept_dir\", concept_dir)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        l_causal = hidden_states @ self.sqrt_cov_gamma\n",
    "        l_causal[:, -1, :] = l_causal[:, -1, :] + self.alpha * self.concept_dir\n",
    "        return l_causal @ self.lm_head_g.T\n",
    "\n",
    "model.lm_head = SteeringHead(g, sqrt_cov_gamma, concept_dir, alpha=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_json('https://raw.githubusercontent.com/donkeyanaphora/STEERING_EXPERIMENTS/refs/heads/main/data/eval.json')\n",
    "\n",
    "questions = []\n",
    "for idx, row in eval_df.iterrows():\n",
    "    q = [\n",
    "        {\"role\": \"user\", \"content\": row.question}, \n",
    "        {\"role\": \"assistant\", \"content\": row.correct_answer}, \n",
    "        {\"role\": \"user\", \"content\": row.challenge}\n",
    "    ]\n",
    "    questions.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer.apply_chat_template(\n",
    "    questions,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    tokenizer_kwargs={\"padding_side\": \"left\"},\n",
    ").to(device)\n",
    "\n",
    "print(batch[\"input_ids\"].shape)\n",
    "print(batch[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df42b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "high, low = 1.2, -1.2\n",
    "alphas = [high, 0, low]\n",
    "\n",
    "outputs = {}\n",
    "for alpha in alphas:\n",
    "    model.lm_head.alpha = alpha\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=600,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        outputs[alpha] = tokenizer.batch_decode(out, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0635732",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(outputs)\n",
    "df['Question'] = [eval_df.iloc[i].question for i in range(len(eval_df))]\n",
    "\n",
    "df_fmt = df.rename(columns={\n",
    "    high: \"High Authority\",\n",
    "    0: \"Baseline\",\n",
    "    low: \"Low Authority\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a87b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "style = \"\"\"\n",
    "<style>\n",
    ".styled-table { width: 100%; border-collapse: collapse; }\n",
    ".styled-table td, .styled-table th { \n",
    "    vertical-align: top; \n",
    "    padding: 12px; \n",
    "    border: 1px solid #ddd;\n",
    "    width: 25%;\n",
    "}\n",
    ".styled-table td { white-space: pre-wrap; }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "html = df_fmt.set_index('Question').to_html(escape=False, classes='styled-table').replace('\\\\n', '<br>')\n",
    "HTML(style + html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8518b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fmt.to_json('results.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
