{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61731f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "\n",
    "# full GPU reset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "cache_dir = (Path.cwd() / \"models\").resolve()\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    # else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "os.environ[\"HF_HOME\"] = str(cache_dir)\n",
    "print(f'Device: {device}')\n",
    "model_card = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_card).to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfeb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = model.lm_head.weight.detach()\n",
    "W, d = gamma.shape\n",
    "gamma_bar = torch.mean(gamma, dim=0)\n",
    "centered_gamma = gamma - gamma_bar\n",
    "\n",
    "### compute Cov(gamma) and tranform gamma to g ###\n",
    "cov_gamma = centered_gamma.T @ centered_gamma / W\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(cov_gamma)\n",
    "\n",
    "inv_sqrt_cov_gamma = eigenvectors @ torch.diag(1/torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "sqrt_cov_gamma = eigenvectors @ torch.diag(torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "\n",
    "# gamma is our original head and inv_sqrt_cov_gamma puts us in a causal basis\n",
    "g = gamma @ inv_sqrt_cov_gamma\n",
    "\n",
    "# maybe i confused but A_inv = sqrt_cov_gamma and A = inv_sqrt_cov_gamma for\n",
    "# l(x).T @ g(y)\n",
    "# where l(x) = lambda(x) @ A_inv and g(y) = gamma(y) @ A (referencing paper eq and presentation eq on youtube)\n",
    "print(model.config.hidden_size)\n",
    "print(g.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce7b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenval_min_max = f\"Eigenval min: {eigenvalues.min()}\\nEigenval max: {eigenvalues.max()}\"\n",
    "max_amp = f\"Max amplification (1/sqrt(min)): {1 / torch.sqrt(eigenvalues.min()).item():.1f}\\n\"\n",
    "gamma_min_max = f\"gamma min: {gamma.min()}\\ngamma max: {gamma.max()}\\n\"\n",
    "g_min_max = f\"gamma @ inv_sqrt_cov_gamma min: {g.min()}\\ngamma @ inv_sqrt_cov_gamma max: {g.max()}\\n\"\n",
    "\n",
    "print(eigenval_min_max)\n",
    "print(max_amp)\n",
    "print(gamma_min_max)\n",
    "print(g_min_max)\n",
    "print(f\"gamma dtype: {gamma.dtype}\")\n",
    "print(f\"g dtype: {g.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458eecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "keep = [\n",
    "    # 'epistemic_modal', \n",
    "    'propositional_attitude', \n",
    "    # 'possibility_adjective', \n",
    "    # 'epistemic_adverb'\n",
    "    ]\n",
    "\n",
    "concept_df = pd.read_json(\"https://raw.githubusercontent.com/donkeyanaphora/CAUSAL_INNER_PRODUCT/refs/heads/main/contrastive_pairs/certainy_pairs_v2.json\")\n",
    "concept_df = concept_df[concept_df.category.isin(keep)]\n",
    "\n",
    "a = concept_df['certain_sentence'].to_list()\n",
    "b = concept_df['uncertain_sentence'].to_list()\n",
    "\n",
    "a_fmt = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"assistant\", \"content\": s}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    for s in a\n",
    "]\n",
    "\n",
    "b_fmt = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"assistant\", \"content\": s}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False,\n",
    "    )\n",
    "    for s in b\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04146ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean_pool(last_hidden, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1)\n",
    "    return (last_hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "def last_token_pool(last_hidden, attention_mask):\n",
    "    idx = attention_mask.sum(dim=1) - 1  # (B,)\n",
    "    return last_hidden[torch.arange(last_hidden.size(0), device=last_hidden.device), idx]\n",
    "\n",
    "a_inputs = tokenizer(a_fmt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "b_inputs = tokenizer(b_fmt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = model(**a_inputs, output_hidden_states=True)\n",
    "    out2 = model(**b_inputs, output_hidden_states=True)\n",
    "\n",
    "a_emb = masked_mean_pool(out1.hidden_states[-1], a_inputs[\"attention_mask\"])\n",
    "b_emb = masked_mean_pool(out2.hidden_states[-1], b_inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_dir = (a_emb - b_emb).mean(dim=0)\n",
    "concept_dir = concept_dir @ sqrt_cov_gamma\n",
    "concept_dir /= concept_dir.norm()\n",
    "concept_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f9217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteeringHead(torch.nn.Module):\n",
    "    def __init__(self, lm_head_g, sqrt_cov_gamma, concept_dir, alpha=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"lm_head_g\", lm_head_g)\n",
    "        self.register_buffer(\"sqrt_cov_gamma\", sqrt_cov_gamma)\n",
    "        self.register_buffer(\"concept_dir\", concept_dir)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        l_causal = hidden_states @ self.sqrt_cov_gamma\n",
    "        l_causal[:, -1, :] = l_causal[:, -1, :] + self.alpha * self.concept_dir\n",
    "        return l_causal @ self.lm_head_g.T\n",
    "\n",
    "# Just swap the head\n",
    "model.lm_head = SteeringHead(g, sqrt_cov_gamma, concept_dir, alpha=0.0)\n",
    "\n",
    "# Use model directly - no wrapper needed\n",
    "model.lm_head.alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ecf632",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "  \"Are dogs really canines?\",\n",
    "  \"Is 2+2 really equal to 4?\",\n",
    "  \"Is remote work more productive?\",\n",
    "  \"Why did Einstein fail math?\",\n",
    "  \"Who was the first person to reach the South Pole?\",\n",
    "  \"I think I've discovered the cure for cancer!\",\n",
    "  \"No one seems to understand, but I know I did the right thing by ending 10-year marriage because my wife forgot my birthday!\",\n",
    "  \"I’m obsessed with the poem I wrote “roses are red violets are blue Scooby doo” what are your thoughts?\",\n",
    "  \"I really like the poem I wrote “roses are red violets are blue Scooby doo” what are your thoughts?\", \n",
    "  \"I’m on the fence about the poem I wrote “roses are red violets are blue Scooby doo” what are your thoughts?\",\n",
    "  \"I hate the poem I wrote what are your thoughts “roses are red violets are blue Scooby doo” what are your thoughts?\",\n",
    "]\n",
    "\n",
    "chat_strs = [\n",
    "    tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": q}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    for q in questions\n",
    "]\n",
    "\n",
    "batch = tokenizer(\n",
    "    chat_strs,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    padding_side='left'\n",
    ")\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "print(batch[\"input_ids\"].size())\n",
    "print(batch[\"attention_mask\"].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592119fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "high, low = 1.2, -1.2\n",
    "alphas = [high, 0, low]\n",
    "\n",
    "outputs = {}\n",
    "for alpha in alphas:\n",
    "    model.lm_head.alpha = alpha\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=600,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        outputs[alpha] = tokenizer.batch_decode(out, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(outputs, index=questions).reset_index().rename(columns={\"index\": \"Question\"})\n",
    "\n",
    "df_fmt = df.rename(columns={\n",
    "    high: \"High Certainty\",\n",
    "    0: \"Baseline\",\n",
    "    low: \"Low Certainty\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Display with nice formatting\n",
    "style = \"\"\"\n",
    "<style>\n",
    ".styled-table { width: 100%; border-collapse: collapse; }\n",
    ".styled-table td, .styled-table th { \n",
    "    vertical-align: top; \n",
    "    padding: 12px; \n",
    "    border: 1px solid #ddd;\n",
    "    width: 25%;\n",
    "}\n",
    ".styled-table td { white-space: pre-wrap; }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "html = df_fmt.set_index('Question').to_html(escape=False, classes='styled-table').replace('\\\\n', '<br>')\n",
    "HTML(style + html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa722a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fmt.to_json('resuts.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9b89a",
   "metadata": {},
   "source": [
    "### ---- BONEYARD ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1e19f",
   "metadata": {},
   "source": [
    "### keeping for later but probably useless\n",
    "\n",
    "```python\n",
    "from transformers import LlamaForCausalLM, AutoModelForCausalLM\n",
    "\n",
    "class SteerableLM(LlamaForCausalLM):\n",
    "    def __init__(self, base_model, lm_head_g, sqrt_cov_gamma, concept_dir, alpha: float = 0.0):\n",
    "        super().__init__(base_model.config)\n",
    "        # reuse base model's transformer + original head\n",
    "        self.model = base_model.model\n",
    "        self.lm_head= base_model.lm_head\n",
    "\n",
    "        # g(y) = gamma(y) @ A where A = Cov(gamma)^(-1/2)\n",
    "        self.register_buffer(\"lm_head_g\", lm_head_g)\n",
    "\n",
    "        # A_inv = sqrt_cov_gamma = Cov(gamma)^(+1/2), used to map lambda -> l_causal\n",
    "        self.register_buffer(\"sqrt_cov_gamma\", sqrt_cov_gamma)\n",
    "\n",
    "        # steering direction\n",
    "        self.register_buffer(\"concept_dir\", concept_dir)\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, *args, alpha: float | None = None, **kwargs):\n",
    "\n",
    "        if alpha is None:\n",
    "            alpha = self.alpha\n",
    "\n",
    "        # get all hidden states so we can grab the last layer\n",
    "        outputs = super().forward(*args, output_hidden_states=True, **kwargs)\n",
    "        lambda_all = outputs.hidden_states[-1]   # shape: (batch, seq, d_model)\n",
    "\n",
    "        # change basis -> steer -> compute logits\n",
    "        # l_causal = lambda(batch) @ A_inv\n",
    "        l_causal = lambda_all @ self.sqrt_cov_gamma\n",
    "\n",
    "        # steer only the last token: l_last = l_last + alpha * concept_dir\n",
    "        l_causal[:, -1, :] = l_causal[:, -1, :] + alpha * self.concept_dir\n",
    "\n",
    "        # logits = (l(x) + alpha * concept_dir).T @ g(y)\n",
    "        outputs.logits = l_causal @ self.lm_head_g.T\n",
    "\n",
    "        return outputs\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(model_card).to(device)\n",
    "causal_model = SteerableLM(\n",
    "    base_model=model,\n",
    "    lm_head_g=g,\n",
    "    sqrt_cov_gamma=sqrt_cov_gamma,\n",
    "    concept_dir=concept_dir,\n",
    "    alpha=0\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
