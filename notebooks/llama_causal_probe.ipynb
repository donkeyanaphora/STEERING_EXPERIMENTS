{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3da3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "\n",
    "# full GPU reset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "cache_dir = (Path.cwd() / \"models\").resolve()\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    # else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "os.environ[\"HF_HOME\"] = str(cache_dir)\n",
    "print(f'Device: {device}')\n",
    "model_card = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_card).to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = model.lm_head.weight.detach()\n",
    "W, d = gamma.shape\n",
    "gamma_bar = torch.mean(gamma, dim=0)\n",
    "centered_gamma = gamma - gamma_bar\n",
    "\n",
    "### compute Cov(gamma) and tranform gamma to g ###\n",
    "cov_gamma = centered_gamma.T @ centered_gamma / W\n",
    "eigenvalues, eigenvectors = torch.linalg.eigh(cov_gamma)\n",
    "\n",
    "inv_sqrt_cov_gamma = eigenvectors @ torch.diag(1/torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "sqrt_cov_gamma = eigenvectors @ torch.diag(torch.sqrt(eigenvalues)) @ eigenvectors.T\n",
    "\n",
    "# gamma is our original head and inv_sqrt_cov_gamma puts us in a causal basis\n",
    "g = gamma @ inv_sqrt_cov_gamma\n",
    "\n",
    "# maybe i confused but A_inv = sqrt_cov_gamma and A = inv_sqrt_cov_gamma for\n",
    "# l(x).T @ g(y)\n",
    "# where l(x) = lambda(x) @ A_inv and g(y) = gamma(y) @ A (referencing paper eq and presentation eq on youtube)\n",
    "print(model.config.hidden_size)\n",
    "print(g.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenval_min_max = f\"Eigenval min: {eigenvalues.min()}\\nEigenval max: {eigenvalues.max()}\"\n",
    "max_amp = f\"Max amplification (1/sqrt(min)): {1 / torch.sqrt(eigenvalues.min()).item():.1f}\\n\"\n",
    "gamma_min_max = f\"gamma min: {gamma.min()}\\ngamma max: {gamma.max()}\\n\"\n",
    "g_min_max = f\"gamma @ inv_sqrt_cov_gamma min: {g.min()}\\ngamma @ inv_sqrt_cov_gamma max: {g.max()}\\n\"\n",
    "\n",
    "print(eigenval_min_max)\n",
    "print(max_amp)\n",
    "print(gamma_min_max)\n",
    "print(g_min_max)\n",
    "print(f\"gamma dtype: {gamma.dtype}\")\n",
    "print(f\"g dtype: {g.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "concept_df = pd.read_json(\"https://raw.githubusercontent.com/donkeyanaphora/STEERING_EXPERIMENTS/refs/heads/main/data/epistemic_privilege_pairs.json\")\n",
    "\n",
    "a_pairs = []\n",
    "b_pairs= []\n",
    "\n",
    "for idx, row in concept_df.iterrows():\n",
    "  a = [\n",
    "        {\"role\": \"assistant\", \"content\": row.prompt}, \n",
    "        {\"role\": \"user\", \"content\": row.high_sentence}\n",
    "      ]\n",
    "  a_pairs.append(a)\n",
    "\n",
    "  b = [\n",
    "        {\"role\": \"assistant\", \"content\": row.prompt}, \n",
    "        {\"role\": \"user\", \"content\": row.low_sentence}\n",
    "      ]\n",
    "  \n",
    "  b_pairs.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_batch = tokenizer.apply_chat_template(\n",
    "    a_pairs,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=False,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    padding=True,\n",
    "    tokenizer_kwargs={\"return_attention_mask\": True},  # safe across versions\n",
    ").to(device)\n",
    "\n",
    "b_batch = tokenizer.apply_chat_template(\n",
    "    b_pairs,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=False,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    padding=True,\n",
    "    tokenizer_kwargs={\"return_attention_mask\": True},  # safe across versions\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# your list of special-token ids (put it on the same device as input_ids)\n",
    "# 882 user 78191 assistant\n",
    "specials = torch.tensor([128000, 128006, 128007, 128009], device=device)\n",
    "\n",
    "def content_mask(batch, specials):\n",
    "    ids = batch[\"input_ids\"]\n",
    "    attn = batch[\"attention_mask\"].bool() \n",
    "    not_special = ~torch.isin(ids, specials)\n",
    "    return attn & not_special\n",
    "\n",
    "def masked_mean_pool(last_hidden, mask):\n",
    "    m = mask.unsqueeze(-1).to(last_hidden.dtype)\n",
    "    return (last_hidden * m).sum(1) / m.sum(1).clamp(min=1)\n",
    "\n",
    "def last_content_token_pool(last_hidden, mask):\n",
    "    # find last position where mask==True (works w/ left or right padding)\n",
    "    B, S, _ = last_hidden.shape\n",
    "    pos = torch.arange(S, device=last_hidden.device).unsqueeze(0).expand(B, S)\n",
    "    idx = torch.where(mask, pos, torch.full_like(pos, -1)).max(dim=1).values.clamp(min=0)\n",
    "    return last_hidden[torch.arange(B, device=last_hidden.device), idx]\n",
    "\n",
    "with torch.no_grad():\n",
    "    out1 = model(**a_batch, output_hidden_states=True)\n",
    "    out2 = model(**b_batch, output_hidden_states=True)\n",
    "\n",
    "mask_a = content_mask(a_batch, specials)\n",
    "mask_b = content_mask(b_batch, specials)\n",
    "\n",
    "a_emb_mean = masked_mean_pool(out1.hidden_states[-1], mask_a)\n",
    "b_emb_mean = masked_mean_pool(out2.hidden_states[-1], mask_b)\n",
    "\n",
    "a_emb_last = last_content_token_pool(out1.hidden_states[-1], mask_a)\n",
    "b_emb_last = last_content_token_pool(out2.hidden_states[-1], mask_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead264c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_dir = (a_emb_mean - b_emb_mean).mean(dim=0)\n",
    "concept_dir = concept_dir @ sqrt_cov_gamma\n",
    "concept_dir /= concept_dir.norm()\n",
    "concept_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteeringHead(torch.nn.Module):\n",
    "    def __init__(self, lm_head_g, sqrt_cov_gamma, concept_dir, alpha=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"lm_head_g\", lm_head_g)\n",
    "        self.register_buffer(\"sqrt_cov_gamma\", sqrt_cov_gamma)\n",
    "        self.register_buffer(\"concept_dir\", concept_dir)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        l_causal = hidden_states @ self.sqrt_cov_gamma\n",
    "        l_causal[:, -1, :] = l_causal[:, -1, :] + self.alpha * self.concept_dir\n",
    "        return l_causal @ self.lm_head_g.T\n",
    "\n",
    "# Just swap the head\n",
    "model.lm_head = SteeringHead(g, sqrt_cov_gamma, concept_dir, alpha=0.0)\n",
    "\n",
    "# Use model directly - no wrapper needed\n",
    "model.lm_head.alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fa42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_json('https://raw.githubusercontent.com/donkeyanaphora/STEERING_EXPERIMENTS/refs/heads/main/data/eval.json')\n",
    "\n",
    "questions = []\n",
    "for idx, row in eval_df.iterrows():\n",
    "  q = [\n",
    "        {\"role\": \"user\", \"content\": row.question}, \n",
    "        {\"role\": \"assistant\", \"content\": row.correct_answer}, \n",
    "        {\"role\":\"user\", \"content\": row.challenge}\n",
    "      ]\n",
    "  questions.append(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer.apply_chat_template(\n",
    "    questions,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    tokenizer_kwargs={\"return_attention_mask\": True, \"padding_side\":'left'},\n",
    ").to(device)\n",
    "\n",
    "print(batch[\"input_ids\"].shape)\n",
    "print(batch[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df42b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "high, low = 1.2, -1.2\n",
    "alphas = [high, 0, low]\n",
    "\n",
    "outputs = {}\n",
    "for alpha in alphas:\n",
    "    model.lm_head.alpha = alpha\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=600,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        outputs[alpha] = tokenizer.batch_decode(out, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0635732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(outputs)\n",
    "df['Question'] = questions\n",
    "\n",
    "df_fmt = df.rename(columns={\n",
    "    high: \"High Authority\",\n",
    "    0: \"Baseline\",\n",
    "    low: \"Low Authority\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a87b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Display with nice formatting\n",
    "style = \"\"\"\n",
    "<style>\n",
    ".styled-table { width: 100%; border-collapse: collapse; }\n",
    ".styled-table td, .styled-table th { \n",
    "    vertical-align: top; \n",
    "    padding: 12px; \n",
    "    border: 1px solid #ddd;\n",
    "    width: 25%;\n",
    "}\n",
    ".styled-table td { white-space: pre-wrap; }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "html = df_fmt.set_index('Question').to_html(escape=False, classes='styled-table').replace('\\\\n', '<br>')\n",
    "HTML(style + html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8518b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fmt.to_json('resuts.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
